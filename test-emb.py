#!/usr/bin/env python3
"""
–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—è —è–∫ –ø—Ä–∞—Ü—é—î —Å–µ–º–∞–Ω—Ç–∏—á–Ω–µ —Ä–æ–∑—É–º—ñ–Ω–Ω—è –≤ –≤–∞—à–æ–º—É –ø–∞–π–ø–ª–∞–π–Ω—ñ
"""

from sentence_transformers import SentenceTransformer
import numpy as np

def demonstrate_semantic_understanding():
    """–ü–æ–∫–∞–∑—É—î —è–∫ –º–æ–¥–µ–ª—å '—Ä–æ–∑—É–º—ñ—î' —Å–ª–æ–≤–∞"""

    print("üß† –ó–∞–≤–∞–Ω—Ç–∞–∂—É—é NLP –º–æ–¥–µ–ª—å (—Ü–µ –º–æ–∂–µ –∑–∞–π–Ω—è—Ç–∏ —Ö–≤–∏–ª–∏–Ω—É)...")

    # –¢–∞ —Å–∞–º–∞ –º–æ–¥–µ–ª—å —â–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –≤–∞—à –ø–∞–π–ø–ª–∞–π–Ω
    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

    print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞! –í–æ–Ω–∞ '—Ä–æ–∑—É–º—ñ—î' –∑–Ω–∞—á–µ–Ω–Ω—è —Å–ª—ñ–≤\n")

    # –¢–µ—Å—Ç–æ–≤—ñ —Ñ—Ä–∞–∑–∏
    test_phrases = [
        "–Æ–ª—ñ—è –¢–∏–º–æ—à–µ–Ω–∫–æ",
        "–ª—ñ–¥–µ—Ä –ø–∞—Ä—Ç—ñ—ó –ë–∞—Ç—å–∫—ñ–≤—â–∏–Ω–∞",
        "–ø–æ–ª—ñ—Ç–∏–∫ –£–∫—Ä–∞—ó–Ω–∏",
        "–µ–∫–æ–Ω–æ–º—ñ—á–Ω—ñ —Ä–µ—Ñ–æ—Ä–º–∏",
        "–ø—Ä–∏–≤–∞—Ç–∏–∑–∞—Ü—ñ—è –ø—ñ–¥–ø—Ä–∏—î–º—Å—Ç–≤",
        "—Å–æ—Ü—ñ–∞–ª—å–Ω–∞ –ø–æ–ª—ñ—Ç–∏–∫–∞",
        "—É–∫—Ä–∞—ó–Ω—Å—å–∫–∏–π —Ä–µ–≥—ñ–æ–Ω",
        "—Ä–æ–∑–≤–∏—Ç–æ–∫ –∫—Ä–∞—ó–Ω–∏",
        "—è–±–ª—É–∫–æ —á–µ—Ä–≤–æ–Ω–µ",
        "–∫—ñ—à–∫–∞ —Å–ø–∏—Ç—å"
    ]

    print("üî¢ –ü–µ—Ä–µ—Ç–≤–æ—Ä—é—é —Å–ª–æ–≤–∞ –≤ —á–∏—Å–ª–æ–≤—ñ –≤–µ–∫—Ç–æ—Ä–∏ (embeddings):")
    print("="*70)

    # –°—Ç–≤–æ—Ä—é—î–º–æ –≤–µ–∫—Ç–æ—Ä–∏ –¥–ª—è –≤—Å—ñ—Ö —Ñ—Ä–∞–∑
    embeddings = model.encode(test_phrases)

    for i, phrase in enumerate(test_phrases):
        vector_sample = embeddings[i][:5]  # –ü–æ–∫–∞–∑—É—î–º–æ —Ç—ñ–ª—å–∫–∏ –ø–µ—Ä—à—ñ 5 —á–∏—Å–µ–ª
        print(f"{phrase:25} ‚Üí [{vector_sample[0]:6.3f}, {vector_sample[1]:6.3f}, {vector_sample[2]:6.3f}, ...]")

    print(f"\nüí° –ö–æ–∂–µ–Ω –≤–µ–∫—Ç–æ—Ä –º–∞—î {len(embeddings[0])} —á–∏—Å–µ–ª!")

    # –î–µ–º–æ–Ω—Å—Ç—Ä—É—î–º–æ —Å—Ö–æ–∂—ñ—Å—Ç—å
    print("\nüéØ –û–±—á–∏—Å–ª—é—é —Å–µ–º–∞–Ω—Ç–∏—á–Ω—É —Å—Ö–æ–∂—ñ—Å—Ç—å:")
    print("="*70)

    target_phrase = "–¢–∏–º–æ—à–µ–Ω–∫–æ –ë–∞—Ç—å–∫—ñ–≤—â–∏–Ω–∞"
    target_embedding = model.encode([target_phrase])[0]

    print(f"üéØ –ó–∞–ø–∏—Ç: '{target_phrase}'")
    print("\n–°—Ö–æ–∂—ñ—Å—Ç—å –∑ —ñ–Ω—à–∏–º–∏ —Ñ—Ä–∞–∑–∞–º–∏:")

    similarities = []
    for i, phrase in enumerate(test_phrases):
        # –û–±—á–∏—Å–ª—é—î–º–æ –∫–æ—Å–∏–Ω—É—Å–Ω—É —Å—Ö–æ–∂—ñ—Å—Ç—å
        similarity = np.dot(target_embedding, embeddings[i]) / (
                np.linalg.norm(target_embedding) * np.linalg.norm(embeddings[i])
        )
        similarities.append((phrase, similarity))

    # –°–æ—Ä—Ç—É—î–º–æ –∑–∞ —Å—Ö–æ–∂—ñ—Å—Ç—é
    similarities.sort(key=lambda x: x[1], reverse=True)

    for phrase, similarity in similarities:
        emoji = "üî•" if similarity > 0.7 else "‚úÖ" if similarity > 0.5 else "ü§î" if similarity > 0.3 else "‚ùå"
        print(f"{emoji} {similarity:.3f} - {phrase}")

    print("\n" + "="*70)
    print("üí° –ü–û–Ø–°–ù–ï–ù–ù–Ø:")
    print("- 1.000 = —ñ–¥–µ–Ω—Ç–∏—á–Ω–∏–π –∑–±—ñ–≥")
    print("- 0.800+ = –¥—É–∂–µ —Å—Ö–æ–∂—ñ –∑–∞ –∑–º—ñ—Å—Ç–æ–º")
    print("- 0.500+ = —Å—Ö–æ–∂—ñ —Ç–µ–º–∞—Ç–∏—á–Ω–æ")
    print("- 0.300+ = —Å–ª–∞–±–∫–∞ —Å—Ö–æ–∂—ñ—Å—Ç—å")
    print("- 0.000- = –∑–æ–≤—Å—ñ–º —Ä—ñ–∑–Ω—ñ")

    print("\nüéì –û–°–¨ –ß–û–ú–£ –≤–∞—à –ø–æ—à—É–∫ –∑–Ω–∞—Ö–æ–¥–∏—Ç—å '—É–∫—Ä–∞—ó–Ω—Å—å–∫—ñ —Ä–µ–≥—ñ–æ–Ω–∏':")
    print("- –ú–æ–¥–µ–ª—å '–±–∞—á–∏—Ç—å' —â–æ —Ü–µ –ø—Ä–æ —É–∫—Ä–∞—ó–Ω—Å—å–∫—É –ø–æ–ª—ñ—Ç–∏–∫—É")
    print("- '–†–µ–≥—ñ–æ–Ω–∞–ª—å–Ω–∞ –ø–æ–ª—ñ—Ç–∏–∫–∞' —Å—Ö–æ–∂–∞ –Ω–∞ '–ø–æ–ª—ñ—Ç–∏—á–Ω—ñ –ø–∞—Ä—Ç—ñ—ó'")
    print("- –í–µ–∫—Ç–æ—Ä–Ω–∏–π –ø–æ—à—É–∫ –∑–Ω–∞—Ö–æ–¥–∏—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–Ω–æ –±–ª–∏–∑—å–∫—ñ —Ç–µ–∫—Å—Ç–∏")

if __name__ == "__main__":
    try:
        demonstrate_semantic_understanding()
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞: {e}")
        print("üí° –ü–µ—Ä–µ–∫–æ–Ω–∞–π—Ç–µ—Å—å —â–æ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ: pip install sentence-transformers")