#!/usr/bin/env python3
"""
Content Analytics Example

This example demonstrates how to use the content analytics features
of the HTML RAG Pipeline to analyze Ukrainian and English content.

Features demonstrated:
- Basic content analytics setup
- Document processing with analytics
- Sentiment analysis
- Controversy detection
- Entity extraction
- Topic classification
- Analytics search and filtering
- Trend analysis
"""

import sys
import logging
from pathlib import Path
from datetime import datetime

# Add the project root to the Python path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.html_rag.core.pipeline import RAGPipeline
from src.html_rag.core.config import PipelineConfig, ContentAnalyticsConfig


# Sample content for testing
SAMPLE_DOCUMENTS = [
    {
        "html": """
        <html>
        <head><title>Political News</title></head>
        <body>
            <h1>–ü—Ä–µ–∑–∏–¥–µ–Ω—Ç –£–∫—Ä–∞—ó–Ω–∏ –∑—É—Å—Ç—Ä—ñ–≤—Å—è –∑ –ª—ñ–¥–µ—Ä–∞–º–∏ –Ñ–°</h1>
            <p>–í–æ–ª–æ–¥–∏–º–∏—Ä –ó–µ–ª–µ–Ω—Å—å–∫–∏–π –ø—Ä–æ–≤—ñ–≤ –≤–∞–∂–ª–∏–≤—ñ –ø–µ—Ä–µ–≥–æ–≤–æ—Ä–∏ –∑ –ø—Ä–µ–¥—Å—Ç–∞–≤–Ω–∏–∫–∞–º–∏ –Ñ–≤—Ä–æ–ø–µ–π—Å—å–∫–æ–≥–æ –°–æ—é–∑—É —É –ö–∏—î–≤—ñ. 
            –ü—ñ–¥ —á–∞—Å –∑—É—Å—Ç—Ä—ñ—á—ñ –æ–±–≥–æ–≤–æ—Ä—é–≤–∞–ª–∏—Å—è –ø–∏—Ç–∞–Ω–Ω—è –Ω–∞—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ—ó –±–µ–∑–ø–µ–∫–∏, –µ–∫–æ–Ω–æ–º—ñ—á–Ω–æ—ó –ø—ñ–¥—Ç—Ä–∏–º–∫–∏ —Ç–∞ 
            —î–≤—Ä–æ–ø–µ–π—Å—å–∫–æ—ó —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—ó –£–∫—Ä–∞—ó–Ω–∏.</p>
            <p>–ü—Ä–µ–∑–∏–¥–µ–Ω—Ç –ø—ñ–¥–∫—Ä–µ—Å–ª–∏–≤ –≤–∞–∂–ª–∏–≤—ñ—Å—Ç—å –º—ñ–∂–Ω–∞—Ä–æ–¥–Ω–æ—ó —Å–ø—ñ–≤–ø—Ä–∞—Ü—ñ —Ç–∞ –≤–∏—Å–ª–æ–≤–∏–≤ –≤–¥—è—á–Ω—ñ—Å—Ç—å –ø–∞—Ä—Ç–Ω–µ—Ä–∞–º 
            –∑–∞ –ø–æ—Å—Ç—ñ–π–Ω—É –ø—ñ–¥—Ç—Ä–∏–º–∫—É —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó –¥–µ—Ä–∂–∞–≤–∏.</p>
        </body>
        </html>
        """,
        "url": "https://example.com/ukraine-eu-meeting"
    },
    {
        "html": """
        <html>
        <head><title>Economic Update</title></head>
        <body>
            <h1>Government Announces New Economic Reforms</h1>
            <p>The Prime Minister unveiled a comprehensive package of economic reforms aimed at 
            promoting sustainable growth and innovation. The reforms include tax incentives for 
            small businesses, investment in renewable energy, and modernization of infrastructure.</p>
            <p>Financial experts have praised the initiative, calling it a positive step towards 
            economic recovery and long-term prosperity.</p>
        </body>
        </html>
        """,
        "url": "https://example.com/economic-reforms"
    },
    {
        "html": """
        <html>
        <head><title>Controversial Statement</title></head>
        <body>
            <h1>–°–∫–∞–Ω–¥–∞–ª—å–Ω–∞ –∑–∞—è–≤–∞ –ø–æ–ª—ñ—Ç–∏–∫–∞ –≤–∏–∫–ª–∏–∫–∞–ª–∞ –∫—Ä–∏—Ç–∏–∫—É</h1>
            <p>–í—á–æ—Ä–∞—à–Ω—è –∑–∞—è–≤–∞ –æ–¥–Ω–æ–≥–æ –∑ –ø—Ä–æ–≤—ñ–¥–Ω–∏—Ö –ø–æ–ª—ñ—Ç–∏–∫—ñ–≤ –∫—Ä–∞—ó–Ω–∏ –≤–∏–∫–ª–∏–∫–∞–ª–∞ —Ö–≤–∏–ª—é –∫—Ä–∏—Ç–∏–∫–∏ —Ç–∞ –ø—Ä–æ—Ç–µ—Å—Ç—ñ–≤ 
            –∑ –±–æ–∫—É –æ–ø–æ–∑–∏—Ü—ñ—ó —Ç–∞ –≥—Ä–æ–º–∞–¥—Å—å–∫–∏—Ö –æ—Ä–≥–∞–Ω—ñ–∑–∞—Ü—ñ–π. –û–ø–æ–∑–∏—Ü—ñ—è –∑–≤–∏–Ω—É–≤–∞—Ç–∏–ª–∞ –ø–æ–ª—ñ—Ç–∏–∫–∞ —É –ø–æ—à–∏—Ä–µ–Ω–Ω—ñ 
            –Ω–µ–ø—Ä–∞–≤–¥–∏–≤–æ—ó —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó —Ç–∞ –ø—ñ–¥—Ä–∏–≤—ñ –¥–æ–≤—ñ—Ä–∏ –¥–æ –¥–µ–º–æ–∫—Ä–∞—Ç–∏—á–Ω–∏—Ö —ñ–Ω—Å—Ç–∏—Ç—É—Ç—ñ–≤.</p>
            <p>–¶–µ–π —ñ–Ω—Ü–∏–¥–µ–Ω—Ç –ø—Ä–∏–∑–≤—ñ–≤ –¥–æ –ø–æ–ª—ñ—Ç–∏—á–Ω–æ—ó –Ω–∞–ø—Ä—É–∂–µ–Ω–æ—Å—Ç—ñ —Ç–∞ –≤–∏–º–æ–≥ —â–æ–¥–æ –ø—É–±–ª—ñ—á–Ω–∏—Ö –≤–∏–±–∞—á–µ–Ω—å.</p>
        </body>
        </html>
        """,
        "url": "https://example.com/political-controversy"
    }
]


def setup_logging():
    """Set up logging for the example."""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    

def create_analytics_pipeline():
    """Create a RAG pipeline with content analytics enabled."""
    print("üöÄ Setting up Content Analytics Pipeline...")
    
    # Configure content analytics
    analytics_config = ContentAnalyticsConfig(
        enabled=True,
        controversy_threshold=0.5,  # Lower threshold for demo
        max_entities_per_document=20,
        sentiment_model="cardiffnlp/twitter-roberta-base-sentiment-latest",
        classification_model="facebook/bart-large-mnli"
    )
    
    # Configure main pipeline
    pipeline_config = PipelineConfig(
        collection_name="analytics_demo",
        prefer_basic_cleaning=True,  # Better for Ukrainian content
        enable_metrics=True
    )
    
    # Create pipeline with analytics enabled
    pipeline = RAGPipeline(
        config=pipeline_config,
        enable_content_analytics=True,
        analytics_config=analytics_config
    )
    
    print("‚úÖ Pipeline created successfully!")
    return pipeline


def process_documents_with_analytics(pipeline):
    """Process sample documents and perform content analytics."""
    print("\nüìÑ Processing documents with content analytics...")
    
    results = []
    
    for i, doc in enumerate(SAMPLE_DOCUMENTS, 1):
        print(f"\n--- Processing Document {i} ---")
        print(f"URL: {doc['url']}")
        
        try:
            # Process with analytics enabled
            result = pipeline.process_html(
                raw_html=doc['html'],
                url=doc['url'],
                analyze_content=True  # Force analytics for this document
            )
            
            if result.get('success'):
                print("‚úÖ Document processed successfully")
                
                # Display analytics results if available
                analytics = result.get('content_analytics')
                if analytics:
                    print("üìä Analytics Results:")
                    
                    # Sentiment
                    sentiment = analytics['sentiment']
                    print(f"  Sentiment: {sentiment['label']} (score: {sentiment['score']:.2f})")
                    
                    # Controversy
                    controversy = analytics['controversy']
                    print(f"  Controversy: {controversy['level']} (score: {controversy['score']:.2f})")
                    
                    # Topics
                    topics = analytics['topics']
                    print(f"  Primary Topic: {topics['primary_topic']} (confidence: {topics['confidence']:.2f})")
                    
                    # Entities
                    entities = analytics['entities']
                    print(f"  Entities Found: {len(entities)}")
                    for entity in entities[:3]:  # Show first 3 entities
                        print(f"    - {entity['text']} ({entity['entity_type']})")
                    
                    # Language and Quality
                    print(f"  Language: {analytics['language']}")
                    print(f"  Quality Score: {analytics.get('quality_score', 0):.2f}")
                
                results.append(result)
            else:
                print(f"‚ùå Failed to process document: {result.get('error')}")
                
        except Exception as e:
            print(f"‚ùå Error processing document: {e}")
    
    print(f"\n‚úÖ Processed {len(results)} documents successfully")
    return results


def demonstrate_analytics_search(pipeline):
    """Demonstrate content analytics search capabilities."""
    print("\nüîç Demonstrating Analytics Search Features...")
    
    try:
        # Get analytics summary
        print("\n--- Analytics Summary ---")
        summary = pipeline.get_analytics_summary()
        
        if 'total_documents_analyzed' in summary:
            print(f"üìà Total Documents Analyzed: {summary['total_documents_analyzed']}")
            print(f"üìä Sentiment Distribution: {summary['sentiment_distribution']}")
            print(f"‚ö†Ô∏è  Controversy Distribution: {summary['controversy_distribution']}")
            print(f"üè∑Ô∏è  Top Topics: {list(summary['topic_distribution'].keys())[:5]}")
            print(f"üíØ Average Quality Score: {summary.get('average_quality_score', 0):.2f}")
        else:
            print("üìù No analytics data available yet")
        
        # Search by sentiment
        print("\n--- Search by Sentiment ---")
        positive_docs = pipeline.search_by_sentiment('positive', n_results=5)
        print(f"üôÇ Found {len(positive_docs)} positive documents")
        
        negative_docs = pipeline.search_by_sentiment('negative', n_results=5)
        print(f"üòû Found {len(negative_docs)} negative documents")
        
        # Search by controversy level
        print("\n--- Search by Controversy Level ---")
        high_controversy = pipeline.search_by_controversy_level('high', n_results=5)
        print(f"üî• Found {len(high_controversy)} high controversy documents")
        
        low_controversy = pipeline.search_by_controversy_level('low', n_results=5)
        print(f"üòä Found {len(low_controversy)} low controversy documents")
        
        # Search by topic
        print("\n--- Search by Topic ---")
        political_docs = pipeline.search_by_topic('politics', n_results=5)
        print(f"üèõÔ∏è  Found {len(political_docs)} political documents")
        
        economic_docs = pipeline.search_by_topic('economics', n_results=5)
        print(f"üí∞ Found {len(economic_docs)} economic documents")
        
    except Exception as e:
        print(f"‚ùå Error in analytics search: {e}")


def demonstrate_regular_search(pipeline):
    """Demonstrate regular search with analytics metadata."""
    print("\nüîé Demonstrating Enhanced Search with Analytics...")
    
    search_queries = [
        "–ß–∏ –∞–Ω–æ–Ω—Å—É–≤–∞–≤ —É—Ä—è–¥ –Ω–æ–≤—É –µ–∫–æ–Ω–æ–º—ñ—á–Ω—É —Ä–µ—Ñ–æ—Ä–º—É?"
    ]
    
    for query in search_queries:
        print(f"\n--- Searching for: '{query}' ---")
        
        try:
            results = pipeline.search(query, n_results=3)
            
            print(f"üéØ Found {len(results)} results")
            
            for i, result in enumerate(results, 1):
                print(f"\n  Result {i}:")
                print(f"    URL: {result.get('metadata', {}).get('url', 'Unknown')}")
                print(f"    Similarity: {result.get('similarity_score', 0):.3f}")
                
                # Show analytics metadata if available
                metadata = result.get('metadata', {})
                if metadata.get('analytics_processed'):
                    print(f"    üìä Sentiment: {metadata.get('sentiment_label', 'Unknown')}")
                    print(f"    üìä Controversy: {metadata.get('controversy_level', 'Unknown')}")
                    print(f"    üìä Topic: {metadata.get('primary_topic', 'Unknown')}")
                    print(f"    üìä Quality: {metadata.get('quality_score', 0):.2f}")
                
                # Show snippet
                text_snippet = result.get('text', '')[:150]
                print(f"    üìù Snippet: {text_snippet}...")
        
        except Exception as e:
            print(f"‚ùå Search error: {e}")


def demonstrate_analytics_on_existing(pipeline):
    """Demonstrate running analytics on existing documents."""
    print("\nüîÑ Running Analytics on Existing Documents...")
    
    try:
        # Analyze existing documents that might not have analytics yet
        result = pipeline.analyze_existing_documents(limit=10)
        
        print(f"üìà Analysis Results:")
        print(f"  Total Documents: {result.total_documents}")
        print(f"  Successfully Analyzed: {result.successful}")
        print(f"  Failed: {result.failed}")
        print(f"  Processing Time: {result.processing_time:.2f} seconds")
        
        if result.errors:
            print(f"  ‚ùå Errors: {len(result.errors)}")
            for error in result.errors[:3]:  # Show first 3 errors
                print(f"    - {error}")
        
        if result.warnings:
            print(f"  ‚ö†Ô∏è  Warnings: {len(result.warnings)}")
        
    except Exception as e:
        print(f"‚ùå Error analyzing existing documents: {e}")


def demonstrate_export_functionality(pipeline):
    """Demonstrate exporting documents with analytics."""
    print("\nüíæ Demonstrating Export Functionality...")
    
    try:
        # Export to JSON
        json_file = "analytics_export.json"
        pipeline.export_documents(json_file, format='json')
        print(f"‚úÖ Exported documents to {json_file}")
        
        # Check if file was created
        if Path(json_file).exists():
            file_size = Path(json_file).stat().st_size
            print(f"üìÅ File size: {file_size} bytes")
        
    except Exception as e:
        print(f"‚ùå Export error: {e}")


def main():
    """Main demonstration function."""
    print("üéØ Content Analytics Demo")
    print("=" * 50)
    
    setup_logging()
    
    try:
        # Create pipeline with analytics
        pipeline = create_analytics_pipeline()
        
        # Process documents with analytics
        results = process_documents_with_analytics(pipeline)
        
        if results:
            # Demonstrate analytics search features
            demonstrate_analytics_search(pipeline)
            
            # Demonstrate enhanced search
            demonstrate_regular_search(pipeline)
            
            # Demonstrate analytics on existing documents
            demonstrate_analytics_on_existing(pipeline)
            
            # Demonstrate export
            demonstrate_export_functionality(pipeline)
            
            # Show pipeline statistics
            print("\nüìà Pipeline Statistics:")
            stats = pipeline.get_pipeline_stats()
            print(f"  Vector Store: {stats.get('vector_store', {}).get('document_count', 0)} documents")
            
            analytics_info = stats.get('content_analytics', {})
            print(f"  Analytics Enabled: {analytics_info.get('enabled', False)}")
            print(f"  Analytics Available: {analytics_info.get('available', False)}")
        
        # Cleanup
        print("\nüßπ Cleaning up...")
        pipeline.cleanup()
        
        print("\n‚úÖ Demo completed successfully!")
        print("\nKey Features Demonstrated:")
        print("- ‚úÖ Multi-language content analytics (Ukrainian & English)")
        print("- ‚úÖ Sentiment analysis with confidence scores")
        print("- ‚úÖ Controversy detection with evidence")
        print("- ‚úÖ Named entity extraction and linking")
        print("- ‚úÖ Topic classification")
        print("- ‚úÖ Content quality scoring")
        print("- ‚úÖ Analytics-enhanced search capabilities")
        print("- ‚úÖ Export functionality with analytics metadata")
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  Demo interrupted by user")
    except Exception as e:
        print(f"\n‚ùå Demo error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()